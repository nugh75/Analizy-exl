{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7544e560",
   "metadata": {},
   "source": [
    "# üìä Analisi Interattiva di Commenti Excel con AI\n",
    "\n",
    "**Sistema ottimizzato per l'analisi qualitativa automatica di commenti con AI multi-provider**\n",
    "\n",
    "## üöÄ Quick Start\n",
    "1. **Configura**: Provider AI nel file `.env`\n",
    "2. **Esegui**: Tutte le celle in ordine\n",
    "3. **Seleziona**: Provider e file Excel\n",
    "4. **Analizza**: Con template o prompt personalizzato\n",
    "5. **Esporta**: Risultati in Excel\n",
    "\n",
    "## ‚ö° Funzionalit√† Principali\n",
    "- **Multi-Provider**: Ollama (locale) + OpenRouter (cloud)\n",
    "- **Batch Processing**: 5x pi√π veloce\n",
    "- **Template Intelligenti**: 6 tipi di analisi predefiniti\n",
    "- **Interface Interattiva**: Configurazione guidata\n",
    "- **Export Avanzato**: Report dettagliati\n",
    "\n",
    "üìö **Documentazione completa**: `/docs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1fb87d",
   "metadata": {},
   "source": [
    "## üìö 1. Setup e Configurazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a56f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup completato!\n",
      "üìÅ Moduli utils caricati dalla directory /utils/\n",
      "üìã Documentazione disponibile in /docs/\n",
      "üß™ Test disponibili in /tests/\n"
     ]
    }
   ],
   "source": [
    "# Import librerie principali\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Ricarica moduli utils se necessario\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Import moduli utils con reload per sviluppo\n",
    "try:\n",
    "    from utils.ai_clients import create_llm_instance, test_all_providers\n",
    "    from utils.config_manager import load_config, save_config, get_default_config\n",
    "    from utils.data_parsers import load_excel_file, get_column_info, validate_excel_file\n",
    "    from utils.batch_processor import process_comments_batch, estimate_batch_time\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Errore import: {e}\")\n",
    "    print(\"üîÑ Provo a ricaricare i moduli...\")\n",
    "    \n",
    "    # Rimuovi moduli dalla cache se presenti\n",
    "    modules_to_reload = ['utils.ai_clients', 'utils.config_manager', 'utils.data_parsers', 'utils.batch_processor']\n",
    "    for module in modules_to_reload:\n",
    "        if module in sys.modules:\n",
    "            del sys.modules[module]\n",
    "    \n",
    "    # Riprova import\n",
    "    from utils.ai_clients import create_llm_instance, test_all_providers\n",
    "    from utils.config_manager import load_config, save_config, get_default_config\n",
    "    from utils.data_parsers import load_excel_file, get_column_info, validate_excel_file\n",
    "    from utils.batch_processor import process_comments_batch, estimate_batch_time\n",
    "\n",
    "# Carica configurazione\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"analisi_log.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Setup completato!\")\n",
    "print(\"üìÅ Moduli utils caricati dalla directory /utils/\")\n",
    "print(\"üìã Documentazione disponibile in /docs/\")\n",
    "print(\"üß™ Test disponibili in /tests/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0949e5",
   "metadata": {},
   "source": [
    "## üéõÔ∏è 2. Selezione Provider AI\n",
    "\n",
    "Seleziona e attiva il provider AI configurato nel file `.env`. I provider disponibili verranno rilevati automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f9b564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug Ollama: URL=http://192.168.129.14:11435, Model=mixtral:8x7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 07:52:06,783 - INFO - HTTP Request: POST http://192.168.129.14:11435/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug OpenRouter: Model=nvidia/llama-3.1-nemotron-ultra-253b-v1, API_Key=***8d2cf823\n",
      "üîç VERIFICA STATO PROVIDER AI\n",
      "==================================================\n",
      "üè† Ollama: ‚úÖ Ollama connesso (http://192.168.129.14:11435) con modello mixtral:8x7b\n",
      "üåê OpenRouter: ‚úÖ OpenRouter connesso con modello nvidia/llama-3.1-nemotron-ultra-253b-v1\n",
      "==================================================\n",
      "‚úÖ Provider disponibili: üè† Ollama (Locale/Gratuito), üåê OpenRouter (Cloud/API)\n",
      "üîç VERIFICA STATO PROVIDER AI\n",
      "==================================================\n",
      "üè† Ollama: ‚úÖ Ollama connesso (http://192.168.129.14:11435) con modello mixtral:8x7b\n",
      "üåê OpenRouter: ‚úÖ OpenRouter connesso con modello nvidia/llama-3.1-nemotron-ultra-253b-v1\n",
      "==================================================\n",
      "‚úÖ Provider disponibili: üè† Ollama (Locale/Gratuito), üåê OpenRouter (Cloud/API)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35367dc20ddb4726b21c2b9d246d824d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>üéØ Selezione Provider AI</h3>'), HBox(children=(Dropdown(description='Provider:'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verifica provider disponibili e configurazione interfaccia\n",
    "\n",
    "def refresh_provider_status():\n",
    "    \"\"\"Aggiorna lo stato dei provider AI disponibili\"\"\"\n",
    "    try:\n",
    "        provider_status = test_all_providers()\n",
    "        \n",
    "        # Aggiorna opzioni dropdown\n",
    "        provider_options = []\n",
    "        if provider_status.get('ollama', False):\n",
    "            provider_options.append(('üè† Ollama (Locale/Gratuito)', 'ollama'))\n",
    "        if provider_status.get('openrouter', False):\n",
    "            provider_options.append(('üåê OpenRouter (Cloud/API)', 'openrouter'))\n",
    "        \n",
    "        if not provider_options:\n",
    "            provider_options = [('‚ùå Nessun provider configurato', 'none')]\n",
    "            print(\"‚ö†Ô∏è Nessun provider configurato correttamente.\")\n",
    "            print(\"üìù Verifica il file .env per configurare Ollama o OpenRouter\")\n",
    "        else:\n",
    "            providers_found = [opt[0] for opt in provider_options]\n",
    "            print(f\"‚úÖ Provider disponibili: {', '.join(providers_found)}\")\n",
    "        \n",
    "        return provider_options, provider_status\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Errore verifica provider: {e}\")\n",
    "        return [('‚ùå Errore configurazione', 'error')], {}\n",
    "\n",
    "# Inizializza stato provider\n",
    "provider_options, provider_status = refresh_provider_status()\n",
    "\n",
    "# Widget interfaccia selezione provider\n",
    "provider_dropdown = widgets.Dropdown(\n",
    "    options=provider_options,\n",
    "    description='Provider:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='60%')\n",
    ")\n",
    "\n",
    "apply_provider_button = widgets.Button(\n",
    "    description=\"‚úÖ Attiva Provider\",\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "refresh_button = widgets.Button(\n",
    "    description=\"üîÑ Ricarica\",\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "provider_output = widgets.Output()\n",
    "\n",
    "def on_provider_apply(b):\n",
    "    \"\"\"Attiva il provider selezionato\"\"\"\n",
    "    global llm, AI_PROVIDER\n",
    "    with provider_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if provider_dropdown.value in ['none', 'error']:\n",
    "            print(\"‚ùå Seleziona un provider valido\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            AI_PROVIDER = provider_dropdown.value\n",
    "            llm = create_llm_instance({'provider': AI_PROVIDER})\n",
    "            print(f\"‚úÖ Provider {AI_PROVIDER.upper()} attivato con successo!\")\n",
    "            print(\"üöÄ Pronto per l'analisi!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Errore attivazione provider {AI_PROVIDER}: {e}\")\n",
    "\n",
    "def on_refresh_providers(b):\n",
    "    \"\"\"Ricarica la lista dei provider disponibili\"\"\"\n",
    "    global provider_options, provider_status\n",
    "    with provider_output:\n",
    "        clear_output()\n",
    "        print(\"üîÑ Ricaricamento provider in corso...\")\n",
    "        \n",
    "        # Ricarica file .env\n",
    "        load_dotenv(override=True)\n",
    "        \n",
    "        # Aggiorna stato provider\n",
    "        provider_options, provider_status = refresh_provider_status()\n",
    "        provider_dropdown.options = provider_options\n",
    "\n",
    "apply_provider_button.on_click(on_provider_apply)\n",
    "refresh_button.on_click(on_refresh_providers)\n",
    "\n",
    "# Interfaccia\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üéØ Selezione Provider AI</h3>\"),\n",
    "    widgets.HBox([provider_dropdown, refresh_button]),\n",
    "    apply_provider_button,\n",
    "    provider_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e396e0",
   "metadata": {},
   "source": [
    "### üìù Riferimento Configurazione\n",
    "\n",
    "Se non vedi provider disponibili, configura nel file `.env`:\n",
    "\n",
    "**Ollama (locale)**: `OLLAMA_BASE_URL=http://localhost:11434` + modello installato  \n",
    "**OpenRouter (cloud)**: `OPENROUTER_API_KEY=sk-or-your-key` + modello scelto\n",
    "\n",
    "üìö **Documentazione completa**: `/docs/configurazione.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e754c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß TEST E FIX MODULO AI_CLIENTS\n",
      "========================================\n",
      "üß™ Test diretto funzione test_connection:\n",
      "   OLLAMA_BASE_URL: http://192.168.129.14:11435\n",
      "   OLLAMA_MODEL: mixtral:8x7b\n",
      "üîç Debug Ollama: URL=http://192.168.129.14:11435, Model=mixtral:8x7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 07:58:16,024 - INFO - HTTP Request: POST http://192.168.129.14:11435/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-07-08 07:58:21,698 - INFO - HTTP Request: POST http://192.168.129.14:11435/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-07-08 07:58:21,698 - INFO - HTTP Request: POST http://192.168.129.14:11435/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üè† Ollama: ‚úÖ Ollama connesso (http://192.168.129.14:11435) con modello mixtral:8x7b\n",
      "\n",
      "üîß Test diretto creazione OllamaLLM:\n",
      "   URL: http://192.168.129.14:11435\n",
      "   Model: mixtral:8x7b\n",
      "   ‚úÖ LLM creato con successo!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   ‚úÖ LLM creato con successo!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Test risposta breve\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     response = \u001b[43mllm_direct\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRispondi solo \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mOK\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚úÖ Risposta ricevuta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/langchain_core/language_models/llms.py:389\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    380\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    385\u001b[39m     **kwargs: Any,\n\u001b[32m    386\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    387\u001b[39m     config = ensure_config(config)\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    400\u001b[39m         .text\n\u001b[32m    401\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/langchain_core/language_models/llms.py:766\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    759\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    763\u001b[39m     **kwargs: Any,\n\u001b[32m    764\u001b[39m ) -> LLMResult:\n\u001b[32m    765\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/langchain_core/language_models/llms.py:973\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    959\u001b[39m     run_managers = [\n\u001b[32m    960\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    961\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    971\u001b[39m         )\n\u001b[32m    972\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m    981\u001b[39m     run_managers = [\n\u001b[32m    982\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m    983\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    990\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m    991\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/langchain_core/language_models/llms.py:792\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    782\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    783\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    788\u001b[39m     **kwargs: Any,\n\u001b[32m    789\u001b[39m ) -> LLMResult:\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    791\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    796\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    800\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    801\u001b[39m         )\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    803\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/langchain_ollama/llms.py:313\u001b[39m, in \u001b[36mOllamaLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    311\u001b[39m generations = []\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     generations.append([final_chunk])\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/langchain_ollama/llms.py:281\u001b[39m, in \u001b[36mOllamaLLM._stream_with_aggregation\u001b[39m\u001b[34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_stream_with_aggregation\u001b[39m(\n\u001b[32m    273\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    274\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m     **kwargs: Any,\n\u001b[32m    279\u001b[39m ) -> GenerationChunk:\n\u001b[32m    280\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_generate_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m                \u001b[49m\u001b[43mgeneration_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    287\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/langchain_ollama/llms.py:236\u001b[39m, in \u001b[36mOllamaLLM._create_generate_stream\u001b[39m\u001b[34m(self, prompt, stop, **kwargs)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_generate_stream\u001b[39m(\n\u001b[32m    231\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    232\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    233\u001b[39m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    234\u001b[39m     **kwargs: Any,\n\u001b[32m    235\u001b[39m ) -> Iterator[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.generate(\n\u001b[32m    237\u001b[39m         **\u001b[38;5;28mself\u001b[39m._generate_params(prompt, stop=stop, **kwargs)\n\u001b[32m    238\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/ollama/_client.py:172\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    169\u001b[39m   e.response.read()\n\u001b[32m    170\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m  \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43merror\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpx/_models.py:929\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    927\u001b[39m decoder = LineDecoder()\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpx/_models.py:916\u001b[39m, in \u001b[36mResponse.iter_text\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    914\u001b[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpx/_models.py:897\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    895\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpx/_models.py:951\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    948\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpx/_client.py:153\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpx/_transports/default.py:127\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Analizy-exl/venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# üîß TEST E FIX MODULO AI_CLIENTS\n",
    "\n",
    "print(\"üîß TEST E FIX MODULO AI_CLIENTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Forza ricaricamento modulo\n",
    "import importlib\n",
    "import utils.ai_clients\n",
    "importlib.reload(utils.ai_clients)\n",
    "\n",
    "# Test diretto della funzione test_connection\n",
    "from utils.ai_clients import test_connection\n",
    "\n",
    "print(\"üß™ Test diretto funzione test_connection:\")\n",
    "print(f\"   OLLAMA_BASE_URL: {os.getenv('OLLAMA_BASE_URL')}\")\n",
    "print(f\"   OLLAMA_MODEL: {os.getenv('OLLAMA_MODEL')}\")\n",
    "\n",
    "# Test Ollama\n",
    "ollama_ok, ollama_msg = test_connection('ollama')\n",
    "print(f\"   üè† Ollama: {'‚úÖ' if ollama_ok else '‚ùå'} {ollama_msg}\")\n",
    "\n",
    "# Test diretto creazione con base_url\n",
    "print(\"\\nüîß Test diretto creazione OllamaLLM:\")\n",
    "try:\n",
    "    from langchain_ollama import OllamaLLM\n",
    "    ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
    "    ollama_model = os.getenv('OLLAMA_MODEL', 'mixtral:8x7b')\n",
    "    \n",
    "    print(f\"   URL: {ollama_url}\")\n",
    "    print(f\"   Model: {ollama_model}\")\n",
    "    \n",
    "    # Creazione LLM come nel modulo utils\n",
    "    llm_direct = OllamaLLM(\n",
    "        model=ollama_model,\n",
    "        temperature=0.1,\n",
    "        base_url=ollama_url\n",
    "    )\n",
    "    \n",
    "    print(\"   ‚úÖ LLM creato con successo!\")\n",
    "    \n",
    "    # Test risposta breve\n",
    "    response = llm_direct.invoke(\"Rispondi solo 'OK'\")\n",
    "    print(f\"   ‚úÖ Risposta ricevuta: {response.strip()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Errore: {e}\")\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f590bb6",
   "metadata": {},
   "source": [
    "## üìÅ 3. Caricamento File Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4de4e9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551c46a5a5634835b42804ebd65e8ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>üìÅ Caricamento File Excel</h3>'), Dropdown(description='File Excel:', layout=Lay‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rileva file Excel nella directory\n",
    "excel_files = [f for f in os.listdir('.') if f.endswith(('.xlsx', '.xls'))]\n",
    "\n",
    "if not excel_files:\n",
    "    excel_files = ['‚ùå Nessun file .xlsx trovato']\n",
    "\n",
    "file_dropdown = widgets.Dropdown(\n",
    "    options=excel_files,\n",
    "    description='File Excel:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='60%')\n",
    ")\n",
    "\n",
    "custom_path = widgets.Text(\n",
    "    placeholder='Oppure inserisci percorso personalizzato...',\n",
    "    description='Percorso:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='60%')\n",
    ")\n",
    "\n",
    "load_button = widgets.Button(\n",
    "    description=\"üìÇ Carica File\",\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "column_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Colonna:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='60%')\n",
    ")\n",
    "\n",
    "confirm_column_button = widgets.Button(\n",
    "    description=\"‚úÖ Conferma Colonna\",\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "file_output = widgets.Output()\n",
    "\n",
    "def on_load_file(b):\n",
    "    global df, file_path\n",
    "    with file_output:\n",
    "        clear_output()\n",
    "        \n",
    "        # Determina il percorso del file\n",
    "        if custom_path.value.strip():\n",
    "            file_path = custom_path.value.strip()\n",
    "        else:\n",
    "            if file_dropdown.value.startswith('‚ùå'):\n",
    "                print(\"‚ùå Nessun file selezionato\")\n",
    "                return\n",
    "            file_path = file_dropdown.value\n",
    "        \n",
    "        try:\n",
    "            # Carica e valida file\n",
    "            df = load_excel_file(file_path)\n",
    "            if df is None:\n",
    "                print(f\"‚ùå Impossibile caricare {file_path}\")\n",
    "                return\n",
    "            \n",
    "            # Aggiorna dropdown colonne\n",
    "            columns = get_column_info(df)\n",
    "            column_dropdown.options = list(columns.keys())\n",
    "            \n",
    "            print(f\"‚úÖ File caricato: {file_path}\")\n",
    "            print(f\"üìä Dimensioni: {df.shape[0]} righe, {df.shape[1]} colonne\")\n",
    "            print(f\"üìã Colonne disponibili: {list(columns.keys())}\")\n",
    "            \n",
    "            # Mostra anteprima\n",
    "            print(\"\\nüìã Anteprima dati:\")\n",
    "            display(df.head())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Errore caricamento file: {e}\")\n",
    "\n",
    "def on_confirm_column(b):\n",
    "    global colonna_riferimento\n",
    "    with file_output:\n",
    "        if not column_dropdown.value:\n",
    "            print(\"‚ùå Seleziona una colonna\")\n",
    "            return\n",
    "        \n",
    "        colonna_riferimento = column_dropdown.value\n",
    "        \n",
    "        # Analizza colonna selezionata\n",
    "        non_vuoti = df[colonna_riferimento].notna().sum()\n",
    "        vuoti = df[colonna_riferimento].isna().sum()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Colonna selezionata: {colonna_riferimento}\")\n",
    "        print(f\"üìä Commenti validi: {non_vuoti}\")\n",
    "        print(f\"üìä Celle vuote: {vuoti}\")\n",
    "        \n",
    "        # Mostra esempi\n",
    "        esempi = df[colonna_riferimento].dropna().head(3).tolist()\n",
    "        print(f\"\\nüìù Esempi di commenti:\")\n",
    "        for i, esempio in enumerate(esempi, 1):\n",
    "            print(f\"   {i}. {esempio[:100]}{'...' if len(esempio) > 100 else ''}\")\n",
    "\n",
    "load_button.on_click(on_load_file)\n",
    "confirm_column_button.on_click(on_confirm_column)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üìÅ Caricamento File Excel</h3>\"),\n",
    "    file_dropdown,\n",
    "    custom_path,\n",
    "    load_button,\n",
    "    widgets.HTML(\"<br><h4>üìã Selezione Colonna Commenti</h4>\"),\n",
    "    column_dropdown,\n",
    "    confirm_column_button,\n",
    "    file_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c6498",
   "metadata": {},
   "source": [
    "## üéØ 4. Configurazione Prompt e Analisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf18316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template di prompt predefiniti\n",
    "template_prompts = {\n",
    "    \"sentiment\": {\n",
    "        \"nome\": \"üìä Sentiment Analysis\",\n",
    "        \"prompt\": \"\"\"Analizza il sentiment di questi commenti e classifica in categorie.\n",
    "Identifica: sentiment generale, intensit√† emotiva, emozioni specifiche.\n",
    "Crea categorie di sentiment significative e descrittive.\n",
    "Massimo 12 categorie.\"\"\"\n",
    "    },\n",
    "    \"feedback\": {\n",
    "        \"nome\": \"üí¨ Feedback Prodotto\",\n",
    "        \"prompt\": \"\"\"Analizza questi feedback di prodotto/servizio.\n",
    "Identifica: aspetti positivi, negativi, suggerimenti, priorit√†.\n",
    "Classifica per area di impatto e urgenza di intervento.\n",
    "Massimo 15 categorie.\"\"\"\n",
    "    },\n",
    "    \"supporto\": {\n",
    "        \"nome\": \"üéß Supporto Clienti\",\n",
    "        \"prompt\": \"\"\"Categorizza queste richieste di supporto.\n",
    "Identifica: tipo problema, urgenza, area competenza, complessit√†.\n",
    "Crea categorie per instradamento e prioritizzazione.\n",
    "Massimo 15 categorie.\"\"\"\n",
    "    },\n",
    "    \"bug_report\": {\n",
    "        \"nome\": \"üêõ Bug Reports\",\n",
    "        \"prompt\": \"\"\"Analizza queste segnalazioni di bug.\n",
    "Identifica: severit√†, area coinvolta, riproducibilit√†, impatto.\n",
    "Classifica per priorit√† di risoluzione.\n",
    "Massimo 12 categorie.\"\"\"\n",
    "    },\n",
    "    \"personalizzato\": {\n",
    "        \"nome\": \"üé® Analisi Personalizzata\",\n",
    "        \"prompt\": \"\"\"Analizza questi commenti secondo i criteri specificati.\n",
    "Crea categorie significative e descrittive.\n",
    "Massimo 15 categorie.\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Widget per template\n",
    "template_dropdown = widgets.Dropdown(\n",
    "    options=[(info[\"nome\"], key) for key, info in template_prompts.items()],\n",
    "    description='Template:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='60%')\n",
    ")\n",
    "\n",
    "prompt_editor = widgets.Textarea(\n",
    "    value=template_prompts[\"sentiment\"][\"prompt\"],\n",
    "    placeholder='Inserisci il tuo prompt personalizzato...',\n",
    "    description='Prompt:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%', height='150px')\n",
    ")\n",
    "\n",
    "# Configurazione batch\n",
    "batch_mode_check = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='‚ö° Modalit√† Batch (5x pi√π veloce)',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "batch_size_slider = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=10,\n",
    "    description='Batch Size:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='60%')\n",
    ")\n",
    "\n",
    "confirm_prompt_button = widgets.Button(\n",
    "    description=\"‚úÖ Conferma Configurazione\",\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='250px')\n",
    ")\n",
    "\n",
    "prompt_output = widgets.Output()\n",
    "\n",
    "def update_prompt_from_template(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        template_key = change['new']\n",
    "        prompt_editor.value = template_prompts[template_key][\"prompt\"]\n",
    "\n",
    "def on_confirm_prompt(b):\n",
    "    global prompt_finale, BATCH_MODE, BATCH_SIZE\n",
    "    with prompt_output:\n",
    "        clear_output()\n",
    "        \n",
    "        prompt_finale = prompt_editor.value.strip()\n",
    "        BATCH_MODE = batch_mode_check.value\n",
    "        BATCH_SIZE = batch_size_slider.value\n",
    "        \n",
    "        if not prompt_finale:\n",
    "            print(\"‚ùå Inserisci un prompt\")\n",
    "            return\n",
    "        \n",
    "        print(\"‚úÖ Configurazione confermata:\")\n",
    "        print(f\"üìù Template: {template_prompts[template_dropdown.value]['nome']}\")\n",
    "        print(f\"‚ö° Batch mode: {'Attivo' if BATCH_MODE else 'Disattivo'}\")\n",
    "        if BATCH_MODE:\n",
    "            print(f\"üì¶ Batch size: {BATCH_SIZE} commenti\")\n",
    "        print(f\"üìä Prompt pronto per l'analisi ({len(prompt_finale)} caratteri)\")\n",
    "\n",
    "template_dropdown.observe(update_prompt_from_template)\n",
    "confirm_prompt_button.on_click(on_confirm_prompt)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üéØ Configurazione Prompt</h3>\"),\n",
    "    template_dropdown,\n",
    "    prompt_editor,\n",
    "    widgets.HTML(\"<br><h4>‚ö° Configurazione Performance</h4>\"),\n",
    "    batch_mode_check,\n",
    "    batch_size_slider,\n",
    "    confirm_prompt_button,\n",
    "    prompt_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161811b9",
   "metadata": {},
   "source": [
    "## üöÄ 5. Esecuzione Analisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82eff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar e controlli esecuzione\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progresso:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#2196F3'},\n",
    "    layout=widgets.Layout(width='70%')\n",
    ")\n",
    "\n",
    "status_label = widgets.HTML(value=\"<b>‚è≥ Pronto per l'analisi</b>\")\n",
    "\n",
    "start_button = widgets.Button(\n",
    "    description=\"üöÄ Avvia Analisi\",\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "analysis_output = widgets.Output()\n",
    "\n",
    "def on_start_analysis(b):\n",
    "    global risultati_analisi\n",
    "    \n",
    "    # Verifica prerequisiti\n",
    "    if 'llm' not in globals():\n",
    "        with analysis_output:\n",
    "            print(\"‚ùå Attiva prima un provider AI\")\n",
    "        return\n",
    "    \n",
    "    if 'df' not in globals() or 'colonna_riferimento' not in globals():\n",
    "        with analysis_output:\n",
    "            print(\"‚ùå Carica prima un file Excel e seleziona la colonna\")\n",
    "        return\n",
    "    \n",
    "    if 'prompt_finale' not in globals():\n",
    "        with analysis_output:\n",
    "            print(\"‚ùå Configura prima il prompt\")\n",
    "        return\n",
    "    \n",
    "    with analysis_output:\n",
    "        clear_output()\n",
    "        \n",
    "        try:\n",
    "            # Preparazione dati\n",
    "            comments = df[colonna_riferimento].dropna().tolist()\n",
    "            total_comments = len(comments)\n",
    "            \n",
    "            print(f\"üîç Inizio analisi di {total_comments} commenti\")\n",
    "            print(f\"‚ö° Modalit√†: {'Batch' if BATCH_MODE else 'Singola'}\")\n",
    "            \n",
    "            if BATCH_MODE:\n",
    "                estimated_time = estimate_batch_time(total_comments, BATCH_SIZE)\n",
    "                print(f\"‚è±Ô∏è Tempo stimato: {estimated_time:.1f} secondi\")\n",
    "            \n",
    "            # Reset progress\n",
    "            progress_bar.value = 0\n",
    "            status_label.value = \"<b>üîÑ Analisi in corso...</b>\"\n",
    "            \n",
    "            # Esecuzione analisi\n",
    "            if BATCH_MODE:\n",
    "                # Usa batch processor\n",
    "                def update_progress(current, total):\n",
    "                    progress = int((current / total) * 100)\n",
    "                    progress_bar.value = progress\n",
    "                    status_label.value = f\"<b>‚ö° Batch processing: {progress}% ({current}/{total})</b>\"\n",
    "                \n",
    "                risultati_analisi = process_comments_batch(\n",
    "                    comments, \n",
    "                    prompt_finale, \n",
    "                    llm, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    progress_callback=update_progress\n",
    "                )\n",
    "            else:\n",
    "                # Processing singolo\n",
    "                risultati_analisi = []\n",
    "                for i, comment in enumerate(comments):\n",
    "                    progress = int(((i + 1) / total_comments) * 100)\n",
    "                    progress_bar.value = progress\n",
    "                    status_label.value = f\"<b>üîÑ Processing: {progress}% ({i+1}/{total_comments})</b>\"\n",
    "                    \n",
    "                    try:\n",
    "                        response = llm.invoke(f\"{prompt_finale}\\n\\nCommento: {comment}\")\n",
    "                        risultati_analisi.append(response)\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Errore commento {i+1}: {e}\")\n",
    "                        risultati_analisi.append(\"Errore di analisi\")\n",
    "                    \n",
    "                    time.sleep(0.5)  # Pausa tra richieste\n",
    "            \n",
    "            # Completamento\n",
    "            progress_bar.value = 100\n",
    "            status_label.value = \"<b>‚úÖ Analisi completata!</b>\"\n",
    "            \n",
    "            print(f\"\\nüéâ Analisi completata con successo!\")\n",
    "            print(f\"üìä Commenti processati: {len(risultati_analisi)}\")\n",
    "            print(f\"üìù Prime etichette: {risultati_analisi[:3]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            progress_bar.value = 0\n",
    "            status_label.value = \"<b>‚ùå Errore analisi</b>\"\n",
    "            print(f\"‚ùå Errore durante l'analisi: {e}\")\n",
    "\n",
    "start_button.on_click(on_start_analysis)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üöÄ Esecuzione Analisi</h3>\"),\n",
    "    status_label,\n",
    "    progress_bar,\n",
    "    start_button,\n",
    "    analysis_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ab60e",
   "metadata": {},
   "source": [
    "## üìä 6. Visualizzazione Risultati ed Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57382aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget per visualizzazione e export\n",
    "view_results_button = widgets.Button(\n",
    "    description=\"üëÅÔ∏è Visualizza Risultati\",\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "export_button = widgets.Button(\n",
    "    description=\"üíæ Esporta Excel\",\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "results_output = widgets.Output()\n",
    "\n",
    "def on_view_results(b):\n",
    "    with results_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if 'risultati_analisi' not in globals():\n",
    "            print(\"‚ùå Esegui prima l'analisi\")\n",
    "            return\n",
    "        \n",
    "        # Crea DataFrame risultati\n",
    "        df_results = df.copy()\n",
    "        \n",
    "        # Allinea risultati con DataFrame originale\n",
    "        comments_series = df[colonna_riferimento].dropna()\n",
    "        results_dict = {}\n",
    "        \n",
    "        for i, (idx, comment) in enumerate(comments_series.items()):\n",
    "            if i < len(risultati_analisi):\n",
    "                results_dict[idx] = risultati_analisi[i]\n",
    "            else:\n",
    "                results_dict[idx] = \"Non analizzato\"\n",
    "        \n",
    "        # Aggiungi colonna risultati\n",
    "        df_results['Etichetta_AI'] = df_results.index.map(results_dict).fillna(\"Vuoto\")\n",
    "        \n",
    "        print(\"üìä RISULTATI ANALISI\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìù Commenti totali: {len(df)}\")\n",
    "        print(f\"‚úÖ Commenti analizzati: {len(risultati_analisi)}\")\n",
    "        print(f\"üìã Provider utilizzato: {AI_PROVIDER.upper()}\")\n",
    "        print(f\"‚ö° Modalit√†: {'Batch' if BATCH_MODE else 'Singola'}\")\n",
    "        \n",
    "        # Statistiche etichette\n",
    "        etichette_count = df_results['Etichetta_AI'].value_counts()\n",
    "        print(f\"\\nüè∑Ô∏è DISTRIBUZIONE ETICHETTE (Top 10):\")\n",
    "        for etichetta, count in etichette_count.head(10).items():\n",
    "            percentage = (count / len(df_results)) * 100\n",
    "            print(f\"   {etichetta}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Anteprima risultati\n",
    "        print(f\"\\nüìã ANTEPRIMA RISULTATI:\")\n",
    "        cols_to_show = [colonna_riferimento, 'Etichetta_AI']\n",
    "        display(df_results[cols_to_show].head(10))\n",
    "        \n",
    "        # Salva risultati globali per export\n",
    "        globals()['df_risultati_finali'] = df_results\n",
    "\n",
    "def on_export_results(b):\n",
    "    with results_output:\n",
    "        if 'df_risultati_finali' not in globals():\n",
    "            print(\"‚ùå Visualizza prima i risultati\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Genera nome file con timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            template_name = template_dropdown.value\n",
    "            provider_name = AI_PROVIDER\n",
    "            \n",
    "            output_filename = f\"analisi_{template_name}_{provider_name}_{timestamp}.xlsx\"\n",
    "            \n",
    "            # Esporta con metadati\n",
    "            with pd.ExcelWriter(output_filename, engine='xlsxwriter') as writer:\n",
    "                # Sheet principale con risultati\n",
    "                df_risultati_finali.to_excel(writer, sheet_name='Risultati', index=False)\n",
    "                \n",
    "                # Sheet con metadati\n",
    "                metadata = {\n",
    "                    'Parametro': [\n",
    "                        'File originale', 'Colonna analizzata', 'Provider AI',\n",
    "                        'Template utilizzato', 'Modalit√† processing', \n",
    "                        'Batch size', 'Data analisi', 'Commenti totali',\n",
    "                        'Commenti analizzati'\n",
    "                    ],\n",
    "                    'Valore': [\n",
    "                        file_path, colonna_riferimento, AI_PROVIDER.upper(),\n",
    "                        template_prompts[template_dropdown.value]['nome'],\n",
    "                        'Batch' if BATCH_MODE else 'Singola',\n",
    "                        BATCH_SIZE if BATCH_MODE else 'N/A',\n",
    "                        datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        len(df), len(risultati_analisi)\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                pd.DataFrame(metadata).to_excel(writer, sheet_name='Metadati', index=False)\n",
    "                \n",
    "                # Sheet con statistiche\n",
    "                etichette_stats = df_risultati_finali['Etichetta_AI'].value_counts().reset_index()\n",
    "                etichette_stats.columns = ['Etichetta', 'Frequenza']\n",
    "                etichette_stats['Percentuale'] = (etichette_stats['Frequenza'] / len(df_risultati_finali) * 100).round(2)\n",
    "                etichette_stats.to_excel(writer, sheet_name='Statistiche', index=False)\n",
    "            \n",
    "            print(f\"‚úÖ Risultati esportati in: {output_filename}\")\n",
    "            print(f\"üìä File contiene {len(df_risultati_finali)} righe in 3 sheet\")\n",
    "            print(f\"üìã Sheet: Risultati, Metadati, Statistiche\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Errore durante l'export: {e}\")\n",
    "\n",
    "view_results_button.on_click(on_view_results)\n",
    "export_button.on_click(on_export_results)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üìä Visualizzazione e Export</h3>\"),\n",
    "    widgets.HBox([view_results_button, export_button]),\n",
    "    results_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20abfb",
   "metadata": {},
   "source": [
    "## üîß 7. Configurazione Avanzata (Opzionale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazioni avanzate per utenti esperti\n",
    "advanced_config = widgets.Accordion([\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<h4>‚ö° Parametri Batch Processing</h4>\"),\n",
    "        widgets.IntSlider(value=5, min=1, max=15, description='Max Batch Size:'),\n",
    "        widgets.FloatSlider(value=1.0, min=0.1, max=5.0, description='Delay (sec):'),\n",
    "        widgets.IntSlider(value=3, min=1, max=10, description='Max Retry:')\n",
    "    ]),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<h4>üéØ Parametri Qualit√†</h4>\"),\n",
    "        widgets.FloatSlider(value=0.7, min=0.1, max=1.0, description='Temperature:'),\n",
    "        widgets.FloatSlider(value=0.3, min=0.0, max=1.0, description='Soglia Confidenza:'),\n",
    "        widgets.IntSlider(value=150, min=50, max=500, description='Max Token:')\n",
    "    ]),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<h4>üìä Output e Logging</h4>\"),\n",
    "        widgets.Checkbox(value=True, description='Salva log dettagliato'),\n",
    "        widgets.Checkbox(value=False, description='Debug mode'),\n",
    "        widgets.Dropdown(options=['INFO', 'DEBUG', 'WARNING'], value='INFO', description='Log Level:')\n",
    "    ])\n",
    "])\n",
    "\n",
    "advanced_config.set_title(0, '‚ö° Performance')\n",
    "advanced_config.set_title(1, 'üéØ Qualit√†')\n",
    "advanced_config.set_title(2, 'üìä Output')\n",
    "\n",
    "print(\"üîß Configurazione avanzata disponibile (opzionale)\")\n",
    "display(advanced_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8211b12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Risorse Aggiuntive\n",
    "\n",
    "- **üìñ Documentazione**: Leggi `/docs/guida_utente.md` per la guida completa\n",
    "- **‚öôÔ∏è Configurazione**: Consulta `/docs/configurazione.md` per setup avanzato\n",
    "- **üß™ Test**: Esegui `python tests/test_integration.py` per verificare il sistema\n",
    "- **üîß Utils**: Moduli riutilizzabili in `/utils/` per progetti personalizzati\n",
    "\n",
    "### üöÄ Performance Tips\n",
    "- Usa **Batch Mode** per file con molti commenti (5x pi√π veloce)\n",
    "- **Ollama** per privacy completa e uso intensivo\n",
    "- **OpenRouter** per modelli all'avanguardia\n",
    "- **Template** predefiniti per risultati rapidi e di qualit√†\n",
    "\n",
    "### üéØ Qualit√† Tips  \n",
    "- **Prompt specifici** aumentano la precisione\n",
    "- **Esempi concreti** nel prompt migliorano i risultati\n",
    "- **Anteprima dati** per verificare la qualit√† del file\n",
    "- **Validation** dei risultati prima dell'export finale"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
